{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data - I\n",
    "\n",
    "\n",
    "There are multiple ways of getting data into python, depending on where the data is stored. The simplest case is when you have data in CSV files, but often, you need to get data from other formats, sources and documents, such as text files, relational databases, websites, APIs, PDF documents, etc. \n",
    "\n",
    "In the following sections, you will learn to get data into python from a number of sources. You will learn to:\n",
    "* Get data from text files\n",
    "* Get data from relational databases\n",
    "* Scrape data from websites\n",
    "* Get data from publicly available APIs\n",
    "* Read PDFs into python\n",
    "\n",
    "In the process, you will also learn how to deal with nuances that inevitably come while getting data from various sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Delimited Files\n",
    "\n",
    "Delimited files are usually text files, where columns are separated by delimiters (such as commas, tabs, semicolons etc.) and each new line is a row.\n",
    "\n",
    "For instance, we have the ```companies.txt``` file, where each column is separated by a tab:\n",
    "\n",
    "<img src=\"companies.png\" style=\"height: 500px; width: 600px\">\n",
    "\n",
    "The easiest way to read delimited files is using ```pd.read_csv(filepath, sep, header)``` and specify a separator (delimiter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reading companies file: throws an error\n",
    "# companies = pd.read_csv(\"companies.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error appears because of a decoding issue pandas is facing. The best way to resolve such problems to search for help, for example on stack overflow, where you'd get excellent suggestions from the community.\n",
    "\n",
    "One possible solution to this problem is found here: https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using encoding = \"ISO-8859-1\"\n",
    "companies = pd.read_csv(\"companies.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From Relational Databases\n",
    "\n",
    "Data is commonly stored in RDBMS, and it is easy to get it into Python. We'll use the most common one - MySQL.\n",
    "\n",
    "There are many libraries to connect MySQL and Python, such as pymysql, MySQLdb, etc. All of them follow the following procedure to connect to MySQL:\n",
    "- Create a connection object between MySQL and python\n",
    "- Create a cursor object (you use the cursor to open and close the connection) \n",
    "- Execute the SQL query \n",
    "- Retrive results of the query using methods such as ```fetchone()```, ```fetchall()```, etc.\n",
    "\n",
    "Let' work through an example using PyMySQL. You can install it using ```pip install pymysql```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# create a connection object 'conn'\n",
    "conn = pymysql.connect(host=\"localhost\", # your host, localhost for your local machine\n",
    "                     user=\"root\", # your username, usually \"root\" for localhost\n",
    "                      passwd=\"yourpassword\", # your password\n",
    "                      db=\"world\") # name of the data base; world comes inbuilt with mysql\n",
    "\n",
    "# create a cursor object c\n",
    "c = conn.cursor()\n",
    "\n",
    "# execute a query using c.execute\n",
    "c.execute(\"select * from city;\")\n",
    "\n",
    "# getting the first row of data as a tuple\n",
    "all_rows = c.fetchall()\n",
    "\n",
    "# to get only the first row, use c.fetchone() instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that it returns a tuple of tuples: each row is a tuple\n",
    "print(type(all_rows))\n",
    "\n",
    "# printing the first few rows\n",
    "print(all_rows[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it would be useful to convert the list into a dataframe, since you can now work with dataframes easily. In this case, you can use the ```pd.DataFrame()``` function, and pass the list version of the tuple.\n",
    "\n",
    "```pd.DataFrame(list_of_tuples)``` converts each tuple in the list to a row in the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(all_rows), columns=[\"ID\", \"Name\", \"Country\", \"District\", \"Population\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From Websites\n",
    "\n",
    "Web scraping refers to the art of programmatically getting data from the internet.  One of the coolest features of python is that it makes it easy to scrape websites.\n",
    "\n",
    "In Python 3, the most popular library for web scraping is ```BeautifulSoup```. To use ```BeautifulSoup```, we will also need the ```requests``` module, which basically connects to a given URL and fetches data from it (in HTML format). A web page is basically HTML code, and the main use of ```BeautifulSoup``` is that it helps you parse HTML easily. \n",
    "\n",
    "**Note**: Discussion on HTML syntax is beyond the scope of this module, though even very basic HTML experience should be enough to understand web scraping. \n",
    "\n",
    "#### Use Case - Fetching Mobile App Reviews from Google Playstore\n",
    "\n",
    "Let's say you want to understand why people install and uninstall mobile apps, and why they like or dislike certain apps. A very rich source of app-reviews data is the Google Playstore, where people write their feedback about the app. \n",
    "\n",
    "The reviews of the Facebook messenger app can be found here: https://play.google.com/store/apps/details?id=com.facebook.orca&hl=en\n",
    "\n",
    "We will scrape reviews of the Messenger app, i.e. get them into python, and then you can do some interesting analyses on that.\n",
    "\n",
    "\n",
    "#### Parsing HTML Data using  BeautifulSoup and Requests\n",
    "\n",
    "To start using BeautifulSoup, install it using ```pip install beautifulsoup4```, and load the module bs4 using ```import bs4```. Also, install the requests module using ```pip install requests```.\n",
    "\n",
    "The general procedure to get data from websites is:\n",
    "1. Use ```requests``` to connect to a URL and get data from it\n",
    "2. Create a ```BeautifulSoup``` object \n",
    "3. Get attributes of the ```BeautifulSoup``` object (i.e. the HTML elements that you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "\n",
    "# getting HTML from the Google Play web page\n",
    "url = \"https://play.google.com/store/apps/details?id=com.facebook.orca&hl=en\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# create a bs4 object\n",
    "# To avoid warnings, provide \"html5lib\" explicitly\n",
    "soup = bs4.BeautifulSoup(req.text, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a bs4 object, you can use it to get specific parts of the HTML document. \n",
    "\n",
    "In HTML, the most common elements are either a part of a ```class```, or are assigned an ```id```. A typical HTML code looks like this:\n",
    "\n",
    "\n",
    "```\n",
    "    <body>\n",
    "        <div class=\"some_class\">\n",
    "            <p id=\"a_unique_id\">\n",
    "                A paragraph that you can read on the webpage here.\n",
    "            </p>\n",
    "        </div>\n",
    "    </body>\n",
    "```\n",
    "\n",
    "\n",
    "The easiest way to get specific parts of the webpage is using the ```soup.select()``` method of bs4. For e.g.:\n",
    "- ```soup.select('div')``` selects all the elements inside a 'div' tag\n",
    "- ```soup.select('div > p')``` selects all the ```<p>``` elements within div tags\n",
    "- ```soup.select('.some_class')``` selects elements inside  ```class = \"some_class\"```\n",
    "- ```soup.select('#some_id')``` selects all the elements inside the ```id=\"some_id\"``` element \n",
    "\n",
    "\n",
    "Now, you need to find the ids / classes / divs etc. inside which the reviews are located. To find those, go to the webpage, right click, and choose 'Inspect'. It will open up the HTML code.\n",
    "\n",
    "We'll not go into details, but if you look carefully, all the reviews are located inside a ```<div class=\"review-body\"> ```. So we use that class to fetch the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the text inside class = \"review-body\"\n",
    "reviews = soup.select('.review-body')\n",
    "print(type(reviews))\n",
    "print(len(reviews))\n",
    "print(\"\\n\")\n",
    "\n",
    "# printing an element of the reviews list\n",
    "print(reviews[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ```reviews``` is a list, and each element of the list contains some HTML code + the review string that we want.\n",
    "\n",
    "We still need to subset only the actual review string, and ignore all the other HTML, as shown in the highlighted section below (we want the bold string).\n",
    "\n",
    "```<div class=\"review-body with-review-wrapper\"> <span class=\"review-title\"></span>```\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    "```<div class=\"review-link\" style=\"display:none\"> <a class=\"id-no-nav play-button tiny\" href=\"#\" target=\"_blank\"> Full Review </a> </div> </div>```\n",
    "\n",
    "\n",
    "Now, there are more than one ways to so this. One way is to ```str.split()``` the entire string into two parts using ```</span>``` as the separator (```str.split(\"sep\")``` separates a string into two parts at \"sep\").\n",
    "\n",
    "Then we'll get this:\n",
    "\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    "```<div class=\"review-link\" style=\"display:none\"> <a class=\"id-no-nav play-button tiny\" href=\"#\" target=\"_blank\"> Full Review </a> </div> </div>```\n",
    "\n",
    "And then ```str.split()``` this string again using ```<div class=\"review-link'``` as the separator, so we'll get only the review string, i.e.:\n",
    "\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split using </span>\n",
    "r = reviews[6]\n",
    "print(type(r))\n",
    "\n",
    "# r is a bs4 tag, convert it into string to use str.split() \n",
    "part1 = str(r).split(\"</span>\")\n",
    "print(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split part1 using '<div class=\"review-link' as the separator\n",
    "# and get the first element of the resulting list\n",
    "# we get the review\n",
    "part2 = part1[1].split('<div class=\"review-link')[0]\n",
    "part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after applying ```str.split()``` twice, we get the review string.  Now, we simply apply this sequence of splitting to every element of the ```reviews``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this sequence of splitting to every element of the reviews list\n",
    "# using a list comprehension\n",
    "reviews_text = [str(r).split(\"</span>\")[1].split('<div class=\"review-link')[0] for r in reviews]\n",
    "\n",
    "# printing the first 10 reviews\n",
    "reviews_text[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you can do some interesting analyses on the reviews, e.g. find how many people complain about memory, storage space, built-in camera etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Practice problem: Scraping amazon.in to get shoe price data \n",
    "import pprint\n",
    "\n",
    "url = \"https://www.amazon.in/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=sport+shoes\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# create a bs4 object\n",
    "# To avoid warnings, provide \"html5lib\" explicitly\n",
    "soup = bs4.BeautifulSoup(req.text, \"html5lib\")\n",
    "\n",
    "# get shoe names\n",
    "# shoe_data = soup.select('.a-size-medium')\n",
    "# shoe_data = soup.select('.a-size-small.a-link-normal.a-text-normal')\n",
    "# print(len(shoe_data))\n",
    "# print(pprint.pprint(shoe_data))\n",
    "\n",
    "# get shoe prices\n",
    "shoe_prices = soup.select('.a-price-whole')\n",
    "print(len(shoe_prices))\n",
    "pprint.pprint(shoe_prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
